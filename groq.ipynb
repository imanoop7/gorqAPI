{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aALG78aBK2qi",
        "outputId": "f969d1e5-278c-4785-f48c-9e4aa653185f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# pip install -q groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from groq import Groq"
      ],
      "metadata": {
        "id": "xB8-Y4z-LCfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "client = Groq(\n",
        "    api_key=userdata.get(\"GROQ_API_KEY\")\n",
        ")"
      ],
      "metadata": {
        "id": "5YrQ8S3hLOwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of low latency LLMs\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9mRH_C2LgVh",
        "outputId": "c53d8321-0dd9-491c-b631-0ce71c0c1bfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Low latency Language Learning Models (LLMs) are important in the field of natural language processing (NLP) and artificial intelligence (AI) for several reasons:\n",
            "\n",
            "1. Real-time processing: Low latency models are able to process and respond to inputs in real-time, which is critical in applications such as real-time translation, voice assistants, and live captioning.\n",
            "\n",
            "2. User experience: A delay in processing and responding to user inputs can result in a poor user experience. Low latency models can provide a more seamless and responsive user experience.\n",
            "\n",
            "3. Improved accuracy: Low latency models can improve the accuracy of NLP and AI systems. When models are able to process and respond to inputs quickly, they are able to maintain context and better understand the user's intent.\n",
            "\n",
            "4. Cost-effective: Low latency models can be more cost-effective in the long run as they can process and respond to inputs faster, reducing the amount of computing resources required.\n",
            "\n",
            "5. Competitive advantage: Low latency models can provide a competitive advantage in industries such as finance, where real-time trading decisions can have a significant impact on profitability.\n",
            "\n",
            "6. Safety and Security: In some applications like autonomous vehicles, self-driving cars, and robotics low latency is crucial for safety and security reasons, where real-time decision making is necessary to prevent accidents.\n",
            "\n",
            "In summary, low latency Language Learning Models (LLMs) are important for real-time processing, user experience, accuracy, cost-effectiveness, competitive advantage, and safety and security.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "client = Groq(api_key=userdata.get('GROQ_API_KEY'),)\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"you are a helpful assistant. You will answer the user question as 'a 5 year old kid can understand'\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of low latency LLMs\",\n",
        "        }\n",
        "    ],\n",
        "\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        "    temperature=0.5,\n",
        "    max_tokens=1024,\n",
        "    top_p=1,\n",
        "    stop=None,\n",
        "    stream=False,\n",
        ")\n",
        "print(chat_completion.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svzD3SzBL16q",
        "outputId": "d8968a41-29ac-469b-a1da-abbc8d8727dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, I'd be happy to explain that in a simple way!\n",
            "\n",
            "Imagine you're playing a game where you press a button and something happens on the screen. If there's a long delay between when you press the button and when you see the result, it can make the game feel slow and frustrating, right?\n",
            "\n",
            "In this situation, the \"low latency\" is how quickly the game responds to your button press. Low latency means a short delay, which makes the game feel more smooth and responsive.\n",
            "\n",
            "Now, let's talk about LLMs, or \"low latency machine learning models\". These are like the game, and the buttons you press are like the data that the model uses to make predictions or decisions.\n",
            "\n",
            "Just like with the game, if the LLM has low latency, it means it can quickly and accurately make predictions or decisions based on the data it's given. This is important because in many situations, like self-driving cars or medical diagnoses, we need the model to make quick and accurate decisions to keep us safe.\n",
            "\n",
            "So, in short, low latency LLMs are important because they can quickly and accurately make predictions or decisions based on data, which is important in many real-world applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s3qpXEZDM5e5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}